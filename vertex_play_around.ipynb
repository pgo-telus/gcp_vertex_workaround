{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6243d39-dd17-4c2b-ab5f-01cff12d63d9",
   "metadata": {},
   "source": [
    "# Vertex AI - Bucket play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41213e-3052-49b7-af6d-949e281efba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global modules\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "import google.oauth2.credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "\n",
    "# Set global vars\n",
    "d_project_config = safe_load(Path(os.getcwd()).open())\n",
    "\n",
    "# Import local modules\n",
    "from gcp import connect_bq_services, connect_storage_services\n",
    "\n",
    "# Connect to google services\n",
    "bq_client = connect_bq_services(d_project_config['gcp-project-name'])\n",
    "storage_client = connect_storage_services(d_project_config['gcp-project-name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8535b-81ef-448a-98f8-f8eceaa9fd70",
   "metadata": {},
   "source": [
    "## GCP bucket workaround "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f61824-d15d-4ba2-8fa7-6097adecd0fb",
   "metadata": {},
   "source": [
    "### 0. Test util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd19be-ac83-4b19-a316-7525f2506a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name):\n",
    "    \"\"\"List blob in a given bucket\"\"\"\n",
    "    blobs, n_blob = storage_client.list_blobs(d_project_config['bucket']), 0\n",
    "    for i, blob in enumerate(blobs):\n",
    "        print(f\"{i} - {blob.name}\")\n",
    "        n_blob += 1\n",
    "    \n",
    "    print(f\"{n_blob} objects contained in bucket {bucket_name}\")\n",
    "\n",
    "# Test objects\n",
    "d_obj_1 = {'foo': 0, 'bar': \"test\"}\n",
    "print(f'obj1:\\n {d_obj_1}')\n",
    "\n",
    "df_obj_2 = pd.DataFrame(np.random.randn(10, 3), columns=['colA', 'colB', 'colC'])\n",
    "print(f'obj2:\\n {df_obj_2.head(2)}')\n",
    "\n",
    "l_object_3 = ['foo', 'bar', 'foobar']\n",
    "print(f'obj3:\\n {l_object_3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f98e6-c2e9-4bcd-ad36-d8c30ca5ffa9",
   "metadata": {},
   "source": [
    "### 1. Load object to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c47a68-5ed6-480b-a647-04623cdc4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blobs(d_project_config['bucket'])\n",
    "\n",
    "# Init bucket\n",
    "bucket = storage_client.bucket(d_project_config['bucket'])\n",
    "\n",
    "# Load a python object (Class-like)\n",
    "bucket.blob('dir_test/obj1.pickle')\\\n",
    "    .upload_from_string(data=pickle.dumps(d_obj_1), content_type='application/octet-stream')\n",
    "\n",
    "# Load a dataframe to csv\n",
    "bucket.blob('obj2.csv')\\\n",
    "    .upload_from_string(data=df_obj_2.to_csv(index=False), content_type='text/csv')\n",
    "\n",
    "# Load a plain text file\n",
    "bucket.blob('obj3')\\\n",
    "    .upload_from_string(data='\\n'.join(l_object_3), content_type='text/plain')\n",
    "\n",
    "# List object after load\n",
    "list_blobs(d_project_config['bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835494b-e021-482f-891e-99b85b566cf7",
   "metadata": {},
   "source": [
    "### 2. Extract object from bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee183d2-c9c8-4427-be13-23c853100e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pickle file\n",
    "local_path = f'/tmp/bucket_content.pickle'\n",
    "\n",
    "# Download locally\n",
    "try:\n",
    "    bucket.blob(path).download_to_filename(local_path)\n",
    "except (NotFound, FileNotFoundError) as e:\n",
    "    if err_raise:\n",
    "        raise FileNotFoundError()\n",
    "\n",
    "# Read from tmp dir\n",
    "with open(local_path, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract plain text file\n",
    "local_path = f'/tmp/bucket_content'\n",
    "\n",
    "# Download locally\n",
    "try:\n",
    "    bucket.blob(path).download_to_filename(local_path)\n",
    "except (NotFound, FileNotFoundError) as e:\n",
    "    if err_raise:\n",
    "        raise FileNotFoundError()\n",
    "\n",
    "# Read from tmp dir\n",
    "with open(local_path, 'rb') as handle:\n",
    "    data = handle.read()\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV x Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name, path = d_project_config['bucket'], 'obj2.csv'\n",
    "df = pd.read_csv(f'gs://{bucket_name}/{path}')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
